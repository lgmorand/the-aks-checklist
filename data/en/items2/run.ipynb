{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "operations\n",
      "6\n",
      "container\n",
      "8\n",
      "identity_access_management\n",
      "3\n",
      "resource_management\n",
      "3\n",
      "network\n",
      "1\n",
      "disaster_recovery\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "### Step 1 get rid of the cluster setup category as we believe all categories are required for cluster setup. its content will be disbursed to\n",
    "### the relevant category\n",
    "\n",
    "# get the cluster_setup file that will have its content moved to the correct category(files) as we dont need \n",
    "# this category anymore\n",
    "with open('cluster_setup.json') as f:\n",
    "    superHeroSquad = json.load(f)\n",
    "\n",
    "# we need to create a dictionary that maps the correct filename to the index of the entries in cluster_setup.json\n",
    "destination = dict()\n",
    "# the csv file has the correct filename to move the cluster_setup item  to in the correct order. reset index creates a new column\n",
    "# that is the index that will be used in a future step\n",
    "cluster_create_map = pd.read_csv(\"cluster_create_map.csv\").reset_index()\n",
    "# lets get the unique categories we have in the csv file\n",
    "for category in cluster_create_map[\"destination\"].drop_duplicates():\n",
    "    # we will filter for the subset rows that belong to that category\n",
    "    context = cluster_create_map[cluster_create_map[\"destination\"] == category][\"index\"]\n",
    "    # the index we be added to the dictionary with the key being the category name\n",
    "    destination[category] = context.to_list()\n",
    "\n",
    "def append_entries(filename,entries):\n",
    "    \"\"\" function that takes a list of new entries and appends them to \n",
    "    a json file that contains a list\"\"\"\n",
    "    print(filename)\n",
    "    print(len(entries))\n",
    "    filename = filename + \".json\"\n",
    "    # get the current content of the json file\n",
    "    with open(filename) as f:\n",
    "        content = json.load(f)\n",
    "    # append the new content to it \n",
    "    content = content + entries\n",
    "    # write the appended list to the file\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(content, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# for each category we had in the list of cluster_setup items\n",
    "for category, value in destination.items():\n",
    "    entries = []\n",
    "    # for each index value that corresponds to the entry we want to get added to that category\n",
    "    for entry in destination[category]:\n",
    "        # add the cluster setup entry that corresponds to that index to a list. this list of cluster setup \n",
    "        # entries can then be added to the list we will add to the correct file\n",
    "        entries.append(superHeroSquad[entry])\n",
    "    # run the function that takes the list and adds it to the correct file\n",
    "    append_entries(category,entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'storage': 'Storage',\n",
       " 'development': 'Application Deployment',\n",
       " 'container': 'Governance and Security',\n",
       " 'disaster_recovery': 'BC and DR',\n",
       " 'network': 'Network Topology and Connectivity',\n",
       " 'resource_management': 'Resource management',\n",
       " 'windows': 'Windows',\n",
       " 'operations': 'Operations',\n",
       " 'identity_access_management': 'Identity and Access Management'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Step 2: take the old items and convert them to the format we need to make it work in FTA checklist\n",
    "import uuid\n",
    "import os\n",
    "# get the mapping table that converts filename to the correct category name in FTA's checklist\n",
    "filename_map = pd.read_csv('filename_map.csv')\n",
    "filename_map.to_dict(\"list\")\n",
    "\n",
    "# convert the mapping table to a mapping dictionary that can more easily be used\n",
    "filename_map_dict = {}\n",
    "for filename, actualname in zip(filename_map['filename'],filename_map['actualname']):\n",
    "    filename_map_dict[filename] = actualname\n",
    "\n",
    "def get_subcategory(tags):\n",
    "    \"\"\"function to get the first tag that isnt all\"\"\"\n",
    "    relevant_tags = [x for x in tags if x != \"all\"]\n",
    "    if len(relevant_tags) > 0:\n",
    "        return relevant_tags[0]\n",
    "    else:\n",
    "        return \"other\"\n",
    "\n",
    "def get_link(entry):\n",
    "    \"\"\" function to get documentation link \"\"\"\n",
    "    if 'documentation' in entry.keys():\n",
    "        try:\n",
    "            context = entry['documentation'][0]\n",
    "        except IndexError:\n",
    "            context = {\"title\":\"none available\",\"url\":\"\"}        \n",
    "    elif 'tools' in entry.keys():\n",
    "        try:\n",
    "            context = entry['tools'][0]\n",
    "        except IndexError:\n",
    "            context = {\"title\":\"none available\",\"url\":\"\"}\n",
    "    else:\n",
    "        context = {\"title\":\"none available\",\"url\":\"\"}\n",
    "    return context[\"url\"]\n",
    "\n",
    "def transform(item,filename):\n",
    "    transformed_item = dict()\n",
    "    transformed_item['text'] =  item['title']\n",
    "    transformed_item[\"description\"] = item[\"description\"]\n",
    "    transformed_item['subcategory'] =  get_subcategory(item['tags'])\n",
    "    try:\n",
    "        transformed_item['category'] = filename_map_dict[filename]\n",
    "    except KeyError:\n",
    "        print(f'cant find {filename} in the dictionary')\n",
    "        pass\n",
    "    transformed_item['guid'] = str(uuid.uuid4())\n",
    "    transformed_item['severity'] = item['priority']\n",
    "    transformed_item['link'] = get_link(item)\n",
    "    return transformed_item\n",
    "\n",
    "dir_path = r'./'\n",
    "# list to store files\n",
    "filenames = []\n",
    "# Iterate directory\n",
    "for file in os.listdir(dir_path):\n",
    "    # check only json files\n",
    "    if file.endswith('.json'):\n",
    "        filenames.append(file)\n",
    "# print(filenames)\n",
    "\n",
    "## get the items in all the different files in the dir_path. start by initializing the transformed\n",
    "# items list\n",
    "items = []\n",
    "# get all the categories available in the mapping table so files in the dir not in the \n",
    "# considered categories (eg cluster_setup) are not considered\n",
    "categories = filename_map_dict.keys()\n",
    "\n",
    "# iterate over the files\n",
    "for file in filenames:\n",
    "    # remove .json from filename\n",
    "    file2 = file.split(\".\")[0]\n",
    "    # check to make sure that the filename is in categories\n",
    "    if file2 in categories:\n",
    "        # get the content of the file\n",
    "        with open(file) as f:\n",
    "            content = json.load(f)\n",
    "        # transform each item in the file to the FT data format\n",
    "        for item in content:\n",
    "            transformed_item = transform(item,file2)\n",
    "            items.append(transformed_item)\n",
    "        print(f\"finished {file2}\")    \n",
    "    else:\n",
    "        print(f'cant find {file2} in the dictionary')\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally we pull the ft data and append it to the transformed data then save it in the ft file\n",
    "with open(\"./ft_data.json\") as f:\n",
    "    content = json.load(f)\n",
    "#     combined_list = content[\"items\"] + items\n",
    "#     content[\"items\"] = combined_list\n",
    "\n",
    "# with open(\"appended_ft_data.json\", 'w', encoding='utf-8') as f:\n",
    "#     json.dump(content, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(content[\"items\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.drop_duplicates(\"link\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_list = content[\"items\"] + items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "content[\"items\"] = combined_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(content[\"items\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(content[\"items\"] )\n",
    "data.to_csv(\"combined.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f5bf244bc7b828d69e99b531c349d842dbdb7a5a820ff61d883606b71f593f4f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
